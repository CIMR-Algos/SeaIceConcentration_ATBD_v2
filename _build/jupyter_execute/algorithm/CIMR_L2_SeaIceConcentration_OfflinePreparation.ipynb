{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe2009",
   "metadata": {},
   "source": [
    "# Offline preparations for L2 Sea Ice Concentration (SIC) and Sea Ice Edge (SIED) algorithms for CIMR\n",
    "\n",
    "This notebook implements a series of \"offline\" preparation steps for the Level-2 SIC3H and SIED3H algorithm for the CIMR mission.\n",
    "\n",
    "We refer to the corresponding [ATBD](https://cimr-algos.github.io/SeaIceConcentration_ATBD/intro.html) and especially the [Baseline Algorithm Definition](https://cimr-algos.github.io/SeaIceConcentration_ATBD/baseline_algorithm_definition.html#baseline-algorithm-definition).\n",
    "\n",
    "Generally, *offline preparations* are steps that are run on a regular basis (e.g. every hours, every day, etc...). They aim at preparing everything needed for the L2 algorithms to run efficiently at once a new L1B file is available. For example, external Auxiliary Data Files (ADFs) can be fetched and pre-processed on an hourly basis by an *offline* chain, to minimize the waiting time when a new L1B product arrives.\n",
    "\n",
    "Specifically, this notebook implements the dynamic tuning of the SIC algorithm against a rolling archive of L1B files, as is the case in the EUMETSAT OSI SAF and ESA CCI processing chains. In an operational, the rolling archive gives access to typically 5-10 days of L1B data (full L1B files or subsets of TBs). In this demonstration prototype, we have to use the few simulated L1B files we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bd61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from glob import glob\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "# modules in the L2 Sea Ice Concentration ATBD (v2) contain software code that implement the SIC algorithm\n",
    "from sirrdp import rrdp_file\n",
    "from pmr_sic import tiepoints as tp\n",
    "from pmr_sic import hybrid_algo\n",
    "\n",
    "# modules to read CIMR L1B files (for the dynamic tuning against an archive of past L1B files)\n",
    "if '/Tools/' not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath('../..') + '/Tools/')\n",
    "from tools import io_handler as io\n",
    "from tools import collocation as coll\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# encoder class for writing JSON files\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    # https://stackoverflow.com/questions/27050108/convert-numpy-type-to-python/27050186#27050186\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, int):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, float):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64eca7ce",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell has a tag 'parameters' and is used for the CLI with papermill\n",
    "l1b_archive_dir = '../data/input'\n",
    "out_dir = '../data/aux'\n",
    "use_oza_adjust = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70af9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = dict()\n",
    "algos['KA'] = {'channels':('tb37v', 'tb37h'),  'target_band':'KA'}\n",
    "algos['CKA'] = {'channels':('tb06v', 'tb37v', 'tb37h'), 'target_band':'C'}\n",
    "algos['KKA'] = {'channels':('tb19v', 'tb37v', 'tb37h'), 'target_band':'KU'}\n",
    "\n",
    "\n",
    "tb_dict = {'tb01':'L','tb06':'C','tb10':'X','tb19':'KU','tb37':'KA',}\n",
    "rev_tb_dict = {v:k for k,v in tb_dict.items()}\n",
    "bands_needed = []\n",
    "for alg in algos.keys():\n",
    "    bands_needed += algos[alg]['channels']\n",
    "bands_needed = list(set([tb_dict[b[:-1]] for b in bands_needed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7117458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the input parameters\n",
    "if not os.path.isdir(out_dir):\n",
    "    raise ValueError(\"The output directory {} does not exist.\".format(out_dir))\n",
    "if not os.path.isdir(l1b_archive_dir):\n",
    "    raise ValueError(\"The l1b archive directory {} does not exist.\".format(l1b_archive_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d39be",
   "metadata": {},
   "source": [
    "**An hybrid dynamic / fixed tuning for the SCEPS Polar Scene 1** The SCEPS Polar Scene 1 (v2.1, early 2024) does not reach to high-enough SICs, its peak is around 92%-94% and there are very few values above 95%. This means we cannot extract TB samples for near-100% SIC conditions with lat-lon box. On the other hand, we know the sea-ice emissivity model used by SCEPS for this version of the scene is tuned against the CCI RRDP (AMSR-E and AMSR2).\n",
    "\n",
    "We thus go for a hybrid approach (dynamic / fixed) for the tuning of our algorithms:\n",
    "* The OW TB samples are selected from the CIMR L1B file, in a lat-lon box over open water conditions.\n",
    "* The CI TB samples are taken from the CCI RRDP (AMSR-E and AMSR2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e704935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OW samples:  15955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CI samples:  13441\n"
     ]
    }
   ],
   "source": [
    "# locate and read the RRDP files\n",
    "rrdp_dir = os.path.abspath('.') + '/sirrdp'\n",
    "area = 'nh'\n",
    "ow_files, ci_files = rrdp_file.find_rrdp_files(rrdp_dir, area=area, years=(2007, 2013, 2018))\n",
    "\n",
    "channels_needed = []\n",
    "for alg in algos.keys():\n",
    "    channels_needed += algos[alg]['channels']\n",
    "channels_needed = set(channels_needed)\n",
    "\n",
    "rrdp_pos = rrdp_file.read_RRDP_files(area, ow_files, ci_files, channels=channels_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f511800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxes of lat/lon: (lat_min, lat_max, lon_min, lon_max)\n",
    "# To simplify, we use lat/lon boxes here, but in an operational\n",
    "# mode the region masks would be defined with more advanced geomasks.\n",
    "OW_box = (73., 76.2, -2., 40)\n",
    "# CI_box = (82.5, 88., 5, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958128b",
   "metadata": {},
   "source": [
    "Locate CIMR L1B files to use in the training (for the time being we only have 1 realistic swath, so we must use it. But in an operational context the offline training would access a rolling archive of past L1B data, typically 5-10 days.\n",
    "\n",
    "To simplify, we only extract OW and CI samples from one swath file from the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d449a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_l1b_fn_patt = 'SCEPS_l1b_sceps_geo_polar_scene_1_unfiltered_tot_minimal_nom_nedt_apc_tot_v2p1.nc'\n",
    "arch_l1b_fn_patt = os.path.join(l1b_archive_dir, arch_l1b_fn_patt)\n",
    "arch_l1b_files = glob(arch_l1b_fn_patt)\n",
    "if len(arch_l1b_files) == 0:\n",
    "    print(\"WARNING : did not find any good L1B file in the archive ({})\".format(arch_l1b_fn_patt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aa8c3",
   "metadata": {},
   "source": [
    "Do the training sample extraction and the storing into tie-point objects for each algorithm in turn (`CKA`, `KKA`, and `KA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8c4275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KA ['KA']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 10 CICE points\n",
      "OW tie-point for KA ALLFEEDS: [139.41070898 210.85681807]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-ALLFEEDS.json\n",
      "OW tie-point for KA FEED0: [139.61863591 211.69713439]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED0.json\n",
      "OW tie-point for KA FEED1: [139.56751169 211.45852874]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED1.json\n",
      "OW tie-point for KA FEED2: [139.51613917 211.21867282]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED2.json\n",
      "OW tie-point for KA FEED3: [139.430838   211.00187565]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED3.json\n",
      "OW tie-point for KA FEED4: [139.39764262 210.75705817]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED4.json\n",
      "OW tie-point for KA FEED5: [139.31441597 210.53854513]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED5.json\n",
      "OW tie-point for KA FEED6: [139.26148008 210.29482339]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED6.json\n",
      "OW tie-point for KA FEED7: [139.22752453 210.07996419]\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CIMRL1B-FEED7.json\n",
      "CKA ['C', 'KA']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 10 CICE points\n",
      "OW tie-point for CKA ALLFEEDS: [157.37265068 139.3836527  210.74229595]\n",
      "CKA SIC configuration file is in ../data/aux/CKA_sic_CIMRL1B-ALLFEEDS.json\n",
      "OW tie-point for CKA FEED0: [158.14674602 139.40309565 210.74556295]\n",
      "CKA SIC configuration file is in ../data/aux/CKA_sic_CIMRL1B-FEED0.json\n",
      "OW tie-point for CKA FEED1: [157.65815072 139.40552123 210.81713763]\n",
      "CKA SIC configuration file is in ../data/aux/CKA_sic_CIMRL1B-FEED1.json\n",
      "OW tie-point for CKA FEED2: [157.12393649 139.41508237 210.83915549]\n",
      "CKA SIC configuration file is in ../data/aux/CKA_sic_CIMRL1B-FEED2.json\n",
      "OW tie-point for CKA FEED3: [156.65400841 139.31590842 210.57733599]\n",
      "CKA SIC configuration file is in ../data/aux/CKA_sic_CIMRL1B-FEED3.json\n",
      "KKA ['KU', 'KA']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Collocate KA -> KU along scan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomasl/Work/DEVALGO/Tools/tools/io_handler.py:480: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  for feed in range(0,self.data[orig_band].dims['n_horns']):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 10 CICE points\n",
      "OW tie-point for KKA ALLFEEDS: [182.34887517 139.40279549 210.85397525]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-ALLFEEDS.json\n",
      "OW tie-point for KKA FEED0: [183.2093201  139.60880568 211.69588408]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED0.json\n",
      "OW tie-point for KKA FEED1: [182.96248328 139.5640257  211.45814876]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED1.json\n",
      "OW tie-point for KKA FEED2: [182.73200623 139.50750784 211.21634999]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED2.json\n",
      "OW tie-point for KKA FEED3: [182.4990057  139.42279606 210.99753822]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED3.json\n",
      "OW tie-point for KKA FEED4: [182.24411856 139.39852843 210.75673674]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED4.json\n",
      "OW tie-point for KKA FEED5: [181.99598277 139.2996849  210.53265552]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED5.json\n",
      "OW tie-point for KKA FEED6: [181.7498074  139.25480565 210.29056396]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED6.json\n",
      "OW tie-point for KKA FEED7: [181.58978474 139.21410147 210.07206771]\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CIMRL1B-FEED7.json\n"
     ]
    }
   ],
   "source": [
    "reload(coll)\n",
    "for algo in algos.keys():\n",
    "    b = algos[algo]['target_band']\n",
    "    bands_needed = list(set([tb_dict[b[:-1]] for b in algos[algo]['channels']]))\n",
    "    print(algo, bands_needed, )\n",
    "    arch_l1b = io.CIMR_L1B(arch_l1b_files[0], keep_calibration_view=True, selected_bands=bands_needed)\n",
    "    \n",
    "    # if asked by the user, apply the (pre-computed) OZA adjustment fields for all bands\n",
    "    if use_oza_adjust:\n",
    "        arch_l1b.apply_OZA_adjustment()\n",
    "    \n",
    "    # coarsen l1b samples along the scanlines with a kernel of 5 (horns are *not* combined)\n",
    "    arch_l1b = arch_l1b.coarsen_along_scanlines(kernel=5)\n",
    "    # collocate TBs to the target band of the algorithm.\n",
    "    if algo == 'KA':\n",
    "        # we use only one frequency: no need for collocation\n",
    "        pass\n",
    "    elif algo == 'KKA':\n",
    "        # the collocation is done along scanlines (to not mix feeds with different OZAs)\n",
    "        arch_l1b = arch_l1b.collocate_along_scan(target_band=b)\n",
    "    elif algo == 'CKA':\n",
    "        # the collocation is done across scanlines and mixes feeds with different OZAs\n",
    "        #    note : here we also mix forward and aft- scans which is maybe not optimal.\n",
    "        arch_l1b = coll.collocate_channels(arch_l1b.data, b, method='nn')\n",
    "    else:\n",
    "        raise NotImplementedError(\"Collocation step missing for {}\".format(algo))\n",
    "    \n",
    "    # how to access the lat and lon depends on the collocation type\n",
    "    if isinstance(arch_l1b, io.CIMR_L1B):\n",
    "        _lat = arch_l1b.data[b].lat\n",
    "        _lon = arch_l1b.data[b].lon\n",
    "    elif str(arch_l1b['_type']) == 'L1C swath':\n",
    "        _lat = arch_l1b['geolocation']['lat']\n",
    "        _lon = arch_l1b['geolocation']['lon']\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized L1 data type.\")\n",
    "    \n",
    "    # Create the masks for OW and CI from the lat-lon boxes\n",
    "    _mask = dict()\n",
    "    _mask['ow'] = (_lat > OW_box[0])*(_lat < OW_box[1])*(_lon > OW_box[2])*(_lon < OW_box[3]).astype('int')\n",
    "    #_mask['ci'] = (_lat > CI_box[0])*(_lat < CI_box[1])*(_lon > CI_box[2])*(_lon < CI_box[3]).astype('int')\n",
    "    \n",
    "    # Prepare for extracting the training samples.We also store the feed number (e.g. 0-7 for KU/KA)\n",
    "    #   to be able to later train algorithm for specific feeds.\n",
    "    dyn_tbs = dict()\n",
    "    dyn_tbs['ow'] = dict()\n",
    "    #dyn_tbs['ci'] = dict()\n",
    "    dyn_tbs_feed = dict()\n",
    "    dyn_tbs_feed['ow'] = None\n",
    "    #dyn_tbs_feed['ci'] = None\n",
    "    \n",
    "    # Extract the brightness temperatures in the OW areas and store in sample dictionaries\n",
    "    for ch in algos[algo]['channels']:\n",
    "        # for each input channel to the algorithm (e.g. tb19v), deduce the\n",
    "        #   name of the variable to be read in the L1B (or L1C) data structure.\n",
    "        bn = tb_dict[ch[:-1]]\n",
    "        pol = ch[-1:]\n",
    "        \n",
    "        if isinstance(arch_l1b, io.CIMR_L1B):\n",
    "            bnstr = ''\n",
    "            if bn != b:\n",
    "                bnstr = '{}_'.format(bn)\n",
    "            tb_n = '{}brightness_temperature_{}'.format(bnstr, pol)\n",
    "            # read and apply the OW or CI mask\n",
    "            for w in ('ow',):\n",
    "                dyn_tbs[w][ch] = arch_l1b.data[b][tb_n].where(_mask[w]).to_masked_array().compressed()\n",
    "                # keep the feed number information only for the target band\n",
    "                if bn == b and dyn_tbs_feed[w] is None:\n",
    "                    dyn_tbs_feed[w] = arch_l1b.data[b]['orig_horn'].where(_mask[w]).to_masked_array().compressed()\n",
    "        elif str(arch_l1b['_type']) == 'L1C swath':\n",
    "            tb_n = 'brightness_temperature_{}'.format(pol)\n",
    "            # read and apply the OW or CI mask\n",
    "            for w in ('ow',):\n",
    "                dyn_tbs[w][ch] = arch_l1b[bn+'_BAND'][tb_n].where(_mask[w]).to_masked_array().compressed()\n",
    "                # keep the feed number information only for the target band\n",
    "                if bn == b and dyn_tbs_feed[w] is None:\n",
    "                    dyn_tbs_feed[w] = arch_l1b[bn+'_BAND']['orig_horn'].where(_mask[w]).to_masked_array().compressed()\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized L1 data type.\")\n",
    "            \n",
    "    #\n",
    "    # TUNING COMBINING ALL FEEDS INTO ONE ALGORITHM\n",
    "    #\n",
    "    \n",
    "    # Transfer the training data in a tie-point object. This involves some processing,\n",
    "    #  like computing the tie-points and their covariance matrices. \n",
    "    ow_tp = tp.OWTiepoint(source='CIMRL1B-ALLFEEDS', tbs=dyn_tbs['ow'])\n",
    "    ci_tp = tp.CICETiepoint(source='rrdp', tbs=rrdp_pos['ci'])\n",
    "    ow_tp.instr = ci_tp.instr = 'CIMR'\n",
    "    print(\"OW tie-point for {} ALLFEEDS: {}\".format(algo, ow_tp.tp))\n",
    "\n",
    "    # Tune the SIC algorithm on these tie-points\n",
    "    tuned_algo = hybrid_algo.HybridSICAlgo(algos[algo]['channels'], ow_tp, ci_tp)\n",
    "\n",
    "    # Store the algorithm on disk (JSON file) for later re-use\n",
    "    json_fn = os.path.join(out_dir,'{}_sic_{}.json'.format(algo.upper(), ow_tp.source.upper()))\n",
    "    with open(json_fn, 'w') as fp_out:\n",
    "        json.dump(tuned_algo.strip().to_dict(), fp_out, indent=4, sort_keys=True, cls=MyEncoder)\n",
    "\n",
    "    print(\"{} SIC configuration file is in {}\".format(algo, json_fn))\n",
    "    \n",
    "    #\n",
    "    # TUNING SEPARATE ALGORITHMS, ONE FOR EACH FEED\n",
    "    #\n",
    "    for feed in range(0, io.n_horns[b]):\n",
    "        FEEDNB = 'FEED{}'.format(feed)\n",
    "        \n",
    "        dyn_tbs_f = dict()\n",
    "        dyn_tbs_f['ow'] = dict()\n",
    "        #dyn_tbs_f['ci'] = dict()\n",
    "        for ch in algos[algo]['channels']:\n",
    "            for w in ('ow',):\n",
    "                dyn_tbs_f[w][ch] = dyn_tbs[w][ch][dyn_tbs_feed[w]==feed]\n",
    "        \n",
    "        ow_tp = tp.OWTiepoint(source='CIMRL1B-{}'.format(FEEDNB), tbs=dyn_tbs_f['ow'])\n",
    "        ci_tp = tp.CICETiepoint(source='CCI-RRDP', tbs=rrdp_pos['ci'])\n",
    "        ow_tp.instr = ci_tp.instr = 'CIMR'\n",
    "        print(\"OW tie-point for {} {}: {}\".format(algo, FEEDNB, ow_tp.tp))\n",
    "        \n",
    "        # Tune the SIC algorithm on these tie-points\n",
    "        tuned_algo = hybrid_algo.HybridSICAlgo(algos[algo]['channels'], ow_tp, ci_tp)\n",
    "\n",
    "        # Store the algorithm on disk (JSON file) for later re-use\n",
    "        json_fn = os.path.join(out_dir,'{}_sic_{}.json'.format(algo.upper(), ow_tp.source.upper()))\n",
    "        with open(json_fn, 'w') as fp_out:\n",
    "            json.dump(tuned_algo.strip().to_dict(), fp_out, indent=4, sort_keys=True, cls=MyEncoder)\n",
    "\n",
    "        print(\"{} SIC configuration file is in {}\".format(algo, json_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa0cc6",
   "metadata": {},
   "source": [
    "At the end, we also run a tuning of each algorithm against the static set of SIC0 and SIC1 data points\n",
    "from the ESA CCI Round Robin Data Package (AMSR-E + AMSR2 TBs). This would not be used in the operational processing, but is used now to demonstrate the impact of dynamic tuning of the algorithms in the Performance Assessment chapter.\n",
    "\n",
    "the training data are taken from the ESA CCI Sea Ice Concentration Round Robin Data Package of Pedersen et al. (2019). The relevant data files as well as routines to parse the files are stored in module siddrp/.\n",
    "\n",
    "Pedersen, Leif Toudal; Saldo, Roberto; Ivanova, Natalia; Kern, Stefan; Heygster, Georg; Tonboe, Rasmus; et al. (2019): Reference dataset for sea ice concentration. figshare. Dataset. https://doi.org/10.6084/m9.figshare.6626549.v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220fcd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 1 OW points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 10 CICE points\n",
      "KA SIC configuration file is in ../data/aux/KA_sic_CCI-RRDP.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 1 OW points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 10 CICE points\n",
      "CKA SIC configuration file is in ../data/aux/CKA_sic_CCI-RRDP.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 1 OW points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select every 10 CICE points\n",
      "KKA SIC configuration file is in ../data/aux/KKA_sic_CCI-RRDP.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run the tuning for each algorithm in turn\n",
    "for algo in algos.keys():\n",
    "    \n",
    "    # Transfer the training data in a tie-point object. This involves some processing,\n",
    "    #  like computing the tie-points and their covariance matrices. \n",
    "    ow_tp = tp.OWTiepoint(source='rrdp', tbs=rrdp_pos['ow'])\n",
    "    ci_tp = tp.CICETiepoint(source='rrdp', tbs=rrdp_pos['ci'])\n",
    "    ow_tp.instr = ci_tp.instr = 'AMSRs'\n",
    "    ow_tp.source = ci_tp.source = 'CCI-RRDP'\n",
    "    \n",
    "    # Tune the SIC algorithm on these tie-points\n",
    "    tuned_algo = hybrid_algo.HybridSICAlgo(algos[algo]['channels'], ow_tp, ci_tp)\n",
    "    \n",
    "    # Store the algorithm on disk (JSON file) for later re-use\n",
    "    json_fn = os.path.join(out_dir,'{}_sic_{}.json'.format(algo.upper(), ow_tp.source.upper()))\n",
    "    with open(json_fn, 'w') as fp_out:\n",
    "        json.dump(tuned_algo.strip().to_dict(), fp_out, indent=4, sort_keys=True, cls=MyEncoder)\n",
    "\n",
    "    print(\"{} SIC configuration file is in {}\".format(algo, json_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e4536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}